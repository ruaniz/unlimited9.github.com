# 케라스 창시자에게 배우는 딥러닝

## 1. 딥러닝이란 무엇인가?

### 1.1 인공 지능과 머신 러닝, 딥러닝
AI(artificial intelligence) > ML(machine learning) > DL(deep learning)

#### 1.1.1 인공지능 : AI(artificial intelligence)
보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동  
어떤 것을 작동시키기 위해 우리가 알고 있는 것 이상으로 컴퓨터가 처리하는 것이 가능한가.
- 심볼릭 AI(symbolic AI)
- 전문가 시스템(expert system)
- 머신 러닝입(machine learning)

#### 1.1.2 머신 러닝 : ML(machine learning)
데이터, 규칙 => 결과 : traditional programming 
데이터, 결과 => 규칙 : machine learningl, training

#### 1.1.3 데이터에서 표현을 학습하기
- 입력 데이터 포인트 : 분석 대상 데이터
- 기대 출력 : 목표 결과
- 알고리즘의 성능을 측정 : 알고리즘의 현재 출력과 기대 출력 간의 차이를 결정  
측정값은 알고리즘의 작동 방식을 교정하기 위한 신호로 다시 피드백되고, 이런 수정 단계를 학습(learning) 이라고 한다.

.머신 러닝에서의 학습Learning이란 더 나은 표현을 찾는 자동화된 과정
.입력 데이터를 의미 있는 데이터로 변환(표현, representation) : machine learning model
.분석을 위헤 데이터를 더 유용한 표현으로 변환 : 좌표변환, 선형 투영(lineal projection), 이동(translation), 비선형 연산 등 
.머신러닝은 변환에 대한 창의력은 없고 가설 공간(hypothesis space)에 정의된 연산 모음을 활용/조사

#### 1.1.4 딥러닝에서 ‘딥’이란 무엇일까? : DL(deep learning)
연속된 층(layer)에서 점진적으로 의미 있는 표현 학습
층 기반 표현 학습(layered representations learning)
계층적 표현 학습(hierarchical representations learning)

cf. 얕은 학습(shallow learning) : 1~2개의 데이터 표현 층을 학습

`신경망(neural network) 모델`
  기본 층을 겹겹이 쌓아 올려 구성

  ref. 인공 신경망(artificial neural network), 생물학적 신경망(biological neural network)

  .뇌 구조에서 영감을 얻어 개발되었으나 딥러닝 모델에서 사용하는 학습 메커니즘과 유사한것이 뇌에 있거나 딥러닝이 뇌를 기반으로 모델링하여 비슷하게 작동하게 작동하는 것은 아니다.
  .신경망 모델을 생물할적 신경망과 연관짓기 보다는 데이터로 부터 표현을 학습하는 수학 모델

  심층 신경망 : 정보가 연속된 필터(filter)를 통과하면서 요구 작업/분석에 대해서 유용하도록 정제되는 다단계 정보 추출 작업

DL은 데이터 표현을 학습하기 위한 다단계 처리 방식

#### 1.1.5 그림 3개로 딥러닝의 작동 원리 이해하기

.입력(input data), 목표 결과(target), 맵핑(mapping) : 
.층(layer) = 데이터 변환 with parameter:가중치(weight)
.가중치(weight) : 층(layer) 파라미터 or 모델(model) 파라미터 
.손실 함수(loss function)/비용 함수(cost function), 목적 함수(objective function)

역전파(backpropagation) 알고리즘을 구현한 옵티마이저(optimizer)는
입력된 데이터를 
1. 층(layer, 데이터 변환)별 가중치(weight)를 적용해 예측(변환)하고
2. 예측(변환) 결과와 목표 결과(target)와 비교/관찰하고
3. 손실 함수를 통해 출력 품질을 측정하여 (차이를 수치화 : 손실점수)
4. 손실 점수가 감소하는 방향으로 가중치를 수정하여
5. 훈련 반복(training loop)  
가중치가 처음에는 random으로 할당되어 손실 점수가 높지만 다양한 샘플을 반복 수행하면서 올바른 방향으로 가중치가 조정되어 손실 점수가 감소  
일반적으로 수천개의 샘풀로 수십번 반복하여 최소한의 손실을 내는 모델이 된다.  


#### 1.1.6 지금까지 딥러닝의 성과
#### 1.1.7 단기간의 과대 선전을 믿지 말자
#### 1.1.8 AI에 대한 전망
단기간의 과대 선전은 믿지 말고 장기 비전을 믿으세요.  
AI가 아직 아무도 감히 생각하지도 못했던 완전한 모습으로 진정한 잠재성을 발휘하려면 어느 정도의 시간이 걸릴지 아무도 모릅니다.  
하지만 AI의 시대는 올 것이고 이 세상을 환상적인 방식으로 변모시킬 것입니다.

### 1.2 딥러닝 이전: 머신 러닝의 간략한 역사

#### 1.2.1 확률적 모델링 : probabilistic modeling
통계학 이론을 데이터 분석 응용으로 초창기 머신 러닝 형태 중 하나고 요즘도 널리 사용

`분류 연산 방식(classification algorithm)`  
- 나이브 베이즈(Naive Bayes)  
입력 데이터의 특성이 모두 독립적이라고 가정하고 베이즈 정리(Bayes’ theorem)를 적용하는 머신 러닝 분류 알고리즘
- 로지스틱 회귀(logistic regression)  
간단하고 다목적으로 활용할 수 있어서 오늘날에도 여전히 유용하며, 데이터 과학자가 분류 작업에 대한 감을 빠르게 얻기 위해 데이터셋에 적용할 첫 번째 알고리즘으로 선택하는 경우가 많음.
(회귀 알고리즘이 아닌 분류 알고리즘. 이름 때문에 혼통하지 말자.)

#### 1.2.2 초창기 신경망
역전파 알고리즘을 재발견하고 신경망에 이를 적용하기 시작하면서 상황이 전환
역전파(backpropagation 알고리즘 : 경사 하강법 최적화를 사용하여 연쇄적으로 변수가 연결된 연산을 훈련하는 방법

#### 1.2.3 커널 방법 : Kernel method
분류 연산 방식(classification algorithm)으로 고차원 비선형 투영(Support Vector Machine, SVM)이 가장 유명(SVM은 분류 뿐아니라 회귀 문제에도 사용가능)
SVM은 2개의 다른 범주에 속한 데이터 포인트 그룹 사이에 좋은 결정 경계(decision boundary)를 찾아 분류

SVM이 결정 경계를 찾는 과정
1. 결정 경계가 하나의 초평면(hyperplane)으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑 (2차원 데이터라면 초평면은 직선)
2. 초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계(하나의 분할 초평면)를 찾기.  
  (이 단계를 마진 최대화(maximizing the margin)라고 하며, 이를 통해 결정 경계가 훈련 데이터셋 이외의 새로운 샘플에 잘 일반화되도록 한다.)

분류 문제를 간단하게 만들어 주기 위해 데이터를 고차원 표현으로 매핑하는 기법이 이론상으로는 좋아보이지만 실제로는 컴퓨터로 구현하기 어려움
그래서 커널 기법(kernel trick)이 등장(커널 방법의 핵심 아이디어)
새롭게 표현된 공간에서 좋은 결정 초평면을 찾기 위해 새로운 공간에 대응하는 데이터 포인트의 좌표를 실제로 구할 필요가 없습니다.
새로운 공간에서의 두 데이터 포인트 사이의 거리를 계산할 수만 있으면 됩니다. 커널 함수(kernel function)를 사용하면 이를 효율적으로 계산할 수 있습니다.
커널 함수는 원본 공간에 있는 두 데이터 포인트를 명시적으로 새로운 표현으로 변환하지 않고 타깃 표현 공간에 위치했을 때의 거리를 매핑해 주는 계산 가능한 연산입니다.
커널 함수는 일반적으로 데이터로부터 학습되지 않고 직접 만들어야 합니다. SVM에서 학습되는 것은 분할 초평면뿐입니다.

SVM이 개발되었을 때 간단한 분류 문제에 대해 최고 수준의 성능을 달성했고 광범위한 이론으로 무장된 몇 안 되는 머신 러닝 방법 중 하나가 되었습니다.
또 수학적으로 깊게 분석하기 용이하여 이론을 이해하고 설명하기 쉽습니다.
이런 유용한 특징 때문에 SVM이 오랫동안 머신 러닝 분야에서 매우 큰 인기를 끌었습니다.

하지만 SVM은 대용량의 데이터셋에 확장되기 어렵고 이미지 분류 같은 지각에 관련된 문제에서 좋은 성능을 내지 못했습니다.
SVM은 얕은 학습 방법이기 때문에 지각에 관련된 문제에 SVM을 적용하려면 먼저 수동으로 유용한 표현을 추출해야 하는데(이런 단계를 특성 공학(feature engineering)이라고 합니다) 이는 매우 어렵고 불안정합니다.

#### 1.2.4 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 머신

- 결정 트리(decision tree)
플로차트flowchart 같은 구조를 가지며 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 예측
결정 트리는 시각화하고 이해하기 쉽다.

- 랜덤 포레스트(Random Forest)
결정 트리 학습에 기초한 것으로 안정적이고 실전에서 유용
서로 다른 결정 트리를 많이 만들고 그 출력을 앙상블하는 방법을 사용
랜덤 포레스트는 다양한 문제에 적용할 수 있고 얕은 학습에 해당하는 어떤 작업에서도 거의 항상 두 번째로 가장 좋은 알고리즘입니다.
잘 알려진 머신 러닝 경연 웹 사이트인 캐글(Kaggle)(http://kaggle.com)이 2010년에 시작되었을 때부터 랜덤 포레스트가 가장 선호하는 알고리즘이 되었습니다. 

- 그래디언트 부스팅 머신(gradient boosting machine)
랜덤 포레스트와 아주 비슷하게 그래디언트 부스팅 머신은 약한 예측 모델인 결정 트리를 앙상블하는 것을 기반으로 하는 머신 러닝 기법
이 알고리즘은 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련함으로써 머신 러닝 모델을 향상하는 방법인 그래디언트 부스팅(gradient boosting)을 사용합니다.
결정 트리에 그래디언트 부스팅 기법을 적용하면 비슷한 성질을 가지면서도 대부분의 경우에 랜덤 포레스트의 성능을 능가하는 모델을 만듭니다.
이 알고리즘이 오늘날 지각에 관련되지 않은 데이터를 다루기 위한 알고리즘 중 최고는 아니지만 가장 뛰어납니다.
딥러닝을 제외하고 캐글 경연 대회에서 가장 많이 사용되는 기법입니다.

#### 1.2.5 다시 신경망으로
- 심층 신경망(deep neural network)  
- 심층 합성곱 신경망(deep convolutional neural network, ConvNet)  

#### 1.2.6 딥러닝의 특징
이전의 머신 러닝 기법은 머신 러닝 방법들로 처리하기 용이하게 사람이 초기 입력 데이터를 여러 방식으로 변환 필요.  
(데이터의 좋은 표현을 수동으로 생성, 특성 공학(feature engineering))  
딥러닝은 이 단계를 자동화하여 특성을 직접 찾는 대신 한 번에 모든 특성을 학습  
이는 머신 러닝 작업 흐름을 매우 단순화시켜 고도의 다단계 작업 과정을 하나의 간단한 엔드-투-엔드(end-to-end) 딥러닝 모델로 대체 가능  

딥러닝의 변환 능력은 모델이 모든 표현 층을 순차적(탐욕적, greedily) 방법이 아니라 동시에 공동으로 학습  

딥러닝이 데이터로부터 학습하는 방법의 두 가지 중요한 특징  
층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다는 것과 이런 점진적인 중간 표현이 공동으로 학습된다는 사실입니다.  
각 층은 상위 층과 하위 층의 표현이 변함에 따라서 함께 바뀝니다.  
이 2개의 특징이 이전의 머신 러닝 접근 방법보다 딥러닝이 훨씬 성공하게 된 이유입니다.

#### 1.2.7 머신 러닝의 최근 동향
오늘날 머신 러닝을 성공적으로 적용하기 위해 알아야 할 두 가지 기술
- 얕은 학습 문제를 위한 그래디언트 부스팅 머신 : XGBoost  
- 지각에 관한 문제를 위한 딥러닝 : 케라스  

### 1.3 왜 딥러닝일까? 왜 지금일까?
컴퓨터 비전에 대한 딥러닝의 두 가지 핵심 아이디어 : 합성곱 신경망과 역전파(1989)  
시계열을 위한 딥러닝의 기본인 LSTM(Long Short-Term Memory) 알고리즘(1997)  

머신 러닝의 진보를 이끈 세 가지 기술적인 힘  
- 하드웨어  
- 데이터셋과 벤치마크(benchmark)  
- 알고리즘 향상  

이 분야는 이론보다 실험을 통해서 성장해 왔기 때문에 새로운 아이디어를 실험할 (또는 종종 기존 아이디어를 확장하기 위해서) 적절한 데이터와 하드웨어가 준비되어 있어야만 알고리즘이 발전  
머신 러닝은 주로 연필과 종이로 발전되는 수학이나 물리학이 아닌 하나의 공학(engineering science)입니다.

1990년대와 2000년대에 걸친 진짜 병목은 데이터와 하드웨어였고 이 기간 동안 인터넷의 시작과 게임 시장이 커지면서 고성능 그래픽 칩이 개발 

#### 1.3.1 하드웨어
CPU는 1990년과 2010년 사이에 거의 5,000배 성능향상.  
하지만 컴퓨터 비전이나 음성 인식에서 사용되는 일반적인 딥러닝 모델 사용에 부족(약 10배 필요)  

NVIDIA와 AMD : 비디오 게임의 그래픽 성능을 높이기 위해 대용량 고속 병렬 칩(그래픽 처리 장치, GPU) 개발  
복잡한 3D 장면을 실시간으로 화면에 그리려는 목적으로만 설계된 저렴한 슈퍼컴퓨터와 같습니다  
NVIDIA가 자사의 GPU 제품을 위한 프로그래밍 인터페이스인 CUDA(https://developer.nvidia.com/about-cuda)를 출시(2007)  
과학 커뮤니티에서 이런 투자의 혜택을 받아 물리 모델링을 시작으로 다양한 병렬 애플리케이션의 대형 CPU 클러스터가 소량의 GPU로 대체되기 시작  
대부분 많은 수의 간단한 행렬 곱셈으로 구성된 심층 신경망도 높은 수준으로 병렬화가 가능  
일부 연구자들이 CUDA를 사용한 신경망 구현을 만들기 시작(2011)

큰 회사들은 NVIDIA Tesla K80처럼 딥러닝을 위해 개발된 GPU 수백 개로 구성된 클러스터에서 딥러닝 모델을 훈련  

이에 더하여 딥러닝 산업은 GPU를 넘어서 더 효율적이고 특화된 딥러닝 칩에 투자하기 시작  
구글은 2016년 I/O 연례 행사에서 텐서 처리 장치(Tensor Processing Unit, TPU) 프로젝트를 공개  
이 칩은 심층 신경망을 실행하기 위해 완전히 새롭게 설계한 것으로 최고 성능을 가진 GPU보다 10배 이상 빠르고 에너지 소비도 더 효율적  

#### 1.3.2 데이터  
지난 20년간 (무어의 법칙Moore’s law에 따라) 저장 장치의 급격한 발전  
머신 러닝을 위한 대량의 데이터셋을 수집하고 배포할 수 있는 인터넷 성장  

- 이미지 데이터셋 : 1,400만 개의 이미지를 1,000개의 범주로 구분해 놓은 ImageNet 데이터셋, 플리커(Flickr)에서 사용자가 붙인 이미지 태그
- 비디오 데이터셋 : 유튜브(YouTube) 비디오
- 자연어 데이터셋 : 위키피디아(Wikipedia)는 자연어 처리 분야에 필요한 핵심 데이터셋

공개 경연 대회(Kaggle(2010), ILSVRC 등)는 연구자들이 경쟁하기 위한 일반적인 기준을 제공하여 최근 딥러닝의 성장 긍정적 영향

#### 1.3.3 알고리즘
하드웨어와 데이터에 이어 2000년대 후반까지는 매우 깊은 심층 신경망을 훈련시킬 수 있는 안정적인 방법을 찾지 못했습니다. 이런 이유로 하나 또는 2개의 층만 사용하는 매우 얕은 신경망만 가능했습니다. SVM과 랜덤 포레스트처럼 잘 훈련된 얕은 학습 방법에 비해 크게 빛을 보지 못했습니다. 깊게 쌓은 층을 통과해서 그래디언트(gradient)를 전파하는 것이 가장 문제였습니다. 신경망을 훈련하기 위한 피드백 신호가 층이 늘어남에 따라 희미해지기 때문입니다.

2009~2010년경에 몇 가지 간단하지만 중요한 알고리즘이 개선되면서 그래디언트를 더 잘 전파되게 만들어 주었고 상황이 바뀌었습니다.
- 신경망의 층에 더 잘 맞는 활성화 함수(activation function)  
- 층별 사전 훈련(pretraining)을 불필요하게 만든 가중치 초기화(weight initialization) 방법  
- RMSProp과 Adam 같은 더 좋은 최적화 방법  

이런 기술의 향상으로 10개 이상의 층을 가진 모델을 훈련시킬 수 있게 되었을 때 비로소 딥러닝이 빛을 발하기 시작했습니다.

2014~2016년 사이에 그래디언트를 더욱 잘 전파할 수 있는 배치 정규화(batch normalization), 잔차 연결(residual connection), 깊이별 분리 합성곱(depthwise separable convolution) 같은 고급 기술들이 개발되었습니다. 요즘에는 층의 깊이가 수천 개인 모델을 처음부터 훈련시킬 수 있습니다.

#### 1.3.4 새로운 투자의 바람  
#### 1.3.5 딥러닝의 대중화  
초창기에 딥러닝 : C++와 CUDA  
대중화된 딥러닝도구  
- 기본 파이썬 스크립트 기술. 심볼릭 텐서 조작 프레임워크, 씨아노(Theano)와 텐서플로(TensorFlow)  
- 러닝 모델을 쉽게 만들 수 있는 케라스  
 
#### 1.3.6 지속될까?
딥러닝의 현재 상태를 AI의 혁명이라고 정의할 수 있는 특징  
- 단순함  
러닝은 특성 공학이 필요하지 않아 복잡하고 불안정한 많은 엔지니어링 과정을 엔드-투-엔드로 훈련시킬 수 있는 모델로 바꾸어 줍니다. 이 모델은 일반적으로 5~6개의 텐서 연산만을 사용하여 만들 수 있습니다.
- 확장성  
딥러닝은 GPU 또는 TPU에서 쉽게 병렬화할 수 있기 때문에 무어의 법칙 혜택을 크게 볼 수 있습니다. 또 딥러닝 모델은 작은 배치(batch) 데이터에서 반복적으로 훈련되기 때문에 어떤 크기의 데이터셋에서도 훈련될 수 있습니다(유일한 병목은 가능한 병렬 계산 능력이지만 무어의 법칙 덕택에 빠르게 그 장벽이 사라지고 있습니다).
- 다용도와 재사용성
이전의 많은 머신 러닝 방법과는 다르게 딥러닝 모델은 처음부터 다시 시작하지 않고 추가되는 데이터로도 훈련할 수 있습니다. 대규모 제품에 사용되는 모델에는 아주 중요한 기능인 연속적인 온라인 학습(online learning)을 가능하게 합니다. 더불어 훈련된 딥러닝 모델은 다른 용도로 쓰일 수 있어 재사용이 가능합니다. 예를 들어 이미지 분류를 위해 훈련된 딥러닝 모델을 비디오 처리 작업 과정에 투입할 수 있습니다. 더 복잡하고 강력한 모델을 만들기 위해 이전의 작업을 재활용할 수 있습니다. 또 아주 작은 데이터셋에도 딥러닝 모델을 적용할 수 있습니다.

과학 혁명 뒤에는 일반적으로 시그모이드(sigmoid) 곡선 형태로 진행됩니다. 초창기에는 매우 빠르게 진행되고 연구자들이 험난한 난관에 부딪히면서 점차 안정되어 나중에는 조금씩 향상됩니다. 2017년의 딥러닝은 시그모이드의 처음 절반 안쪽에 있는 것 같습니다. 앞으로 몇 년 동안 이 혁명은 훨씬 더 빠르게 진행될 것입니다.

 

## 9. Appendix

#### reference site

+ 텐서 플로우 블로그 (Tensor ≈ Blog)  
https://tensorflow.blog

+ 텐서 플로우 블로그 (Tensor ≈ Blog)/케라스 딥러닝  
https://tensorflow.blog/%ec%bc%80%eb%9d%bc%ec%8a%a4-%eb%94%a5%eb%9f%ac%eb%8b%9d/



>
>
>
>
>
>
>
>




## 대규모 데이터 마이닝을 위한 머신 러닝 및 딥 러닝 프레임 워크 및 라이브러리

#### 데이터 마이닝 (DM)
데이터에서 흥미롭고 잠재적으로 유용한 정보를 추출하는 것을 목표로하는 지식 발견 프로세스의 핵심 단계

#### Artificial Intelligence (AI) : 인공 지능
컴퓨터 학습, 자연어 처리 (NLP), 언어 합성, 컴퓨터 비전, 로봇 공학, 센서 분석, 최적화 및 시뮬레이션 등  
컴퓨터가 인간의 행동을 모방 할 수 있도록하는 기술

#### Machine Learning (ML) : 기계 학습
컴퓨터 시스템이 이전 경험 (예 : 데이터 관찰)을 통해 학습하고 주어진 작업에 대한 동작을 개선 할 수 있도록하는 AI 기술의 하위 집합  
ML 기술에는 SVM (Support Vector Machine), 의사 결정 트리, Bayes 학습, k- 평균 군집화, 연관 규칙 학습, 회귀, 신경망 등이 포함

#### (artificial) Neural Networks (NN) : (인공) 신경망
생물학적 신경망에서 느슨하게 영감을 얻은 ML 기술의 하위 집합  
보통 층으로 구성된 인공 뉴런이라고 불리는 연결된 유닛의 집합으로 묘사

#### Deep Learning (DL) : 심층 학습
계산 멀티 레이어 NN을 실현할 수있는 NN의 하위 집합  
일반적인 DL 아키텍처 : DNN (Deep Neural Network), CNN (Convolutional Neural Network), RNN (Recurrent Neural Network), GAN (Generative Adversarial Network) 등


